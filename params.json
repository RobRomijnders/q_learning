{
  "name": "Q learning",
  "tagline": "Implementation of epsilon-greedy q-learning for robot on 2D grid",
  "body": "### Q-learning\r\nThis post implements a Q-learning algorithm with epsilon-greedy approach for a robot in a 2D grid. Q-learning is an approach in reinforcement learning to solve the Bellman equations. Other than for _value iterations_, the state-transition-model can be unknown and based solely on observed state transitions.\r\n\r\n### Our setting\r\nOur setting is a 2D grid, loaded with rewards. Q-learning would also work for a sparse grid with rewards. However, to learn the reinforcement learning algorithms, we would advise to start with a simple grid like this. The imaginary _robot_ starts at state 0. The final state is defined as state 100, where state 1-100 represent the column-wise linear indices of the 10 by 10 grid. \r\n![An example of reward_normal.mat](https://github.com/RobRomijnders/q_learning/blob/master/reward_visualization.png?raw=true)\r\n\r\n### Further improvements\r\nThis code implements Q-learning at its minimal form. For anyone new to Q-learning, the reward matrices and a 10x10 grid form a good first learning experience.\r\nFurther improvements would be:\r\n * Explore the N matrix. The code saves state-action pairs to a big matrix named _N_. Exploring this matrix will giver you lots of intuition regarding the behavior of epsilon-greedy.\r\n * Explore different values for gamma. The example code in _RL_main_github.m_ calls for gamma at 0.5 and 0.9. You'll see that the optimal policy won't reach final state for gamma = 0.5, but it does for gamma=0.9. The threshold lies at gamma=0.8. Interestingly, the optimal policy at gamma=0.7 does reach final state for reward function _reward_long.mat_. Vice versa, optimal policy at gamma=0.9 is the lowest gamma for which the optimal policy reaches final state with reward function _reward_short.mat_.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}