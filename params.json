{
  "name": "Q learning",
  "tagline": "Implementation of epsilon-greedy q-learning for robot on 2D grid",
  "body": "### Q-learning\r\nThis post implements a Q-learning algorithm with epsilon-greedy approach for a robot in a 2D grid. Q-learning is an approach in reinforcement learning to solve the Bellman equations. Other than for _value iterations_, the state-transition-model can be unknown and based solely on observed state transitions.\r\n\r\n### Our setting\r\nOur setting is a 2D grid, loaded with rewards. Q-learning would also work for a sparse grid with rewards. However, to learn the reinforcement learning algorithms, we would advise to start with a simple grid like this. The imaginary _robot_ starts at state 0. The final state is defined as state 100, where state 1-100 represent the column-wise linear indices of the 10 by 10 grid. \r\n![An example of reward_normal.mat](https://github.com/RobRomijnders/q_learning/blob/master/reward_visualization.png?raw=true)\r\n\r\n### Visualizing under the hood\r\nThe Q-learning function in _RL_main_github.m_ calls for some numerical values. That's it. If you want more intuition for the learning procedure, many of the functions take a boolean flag for visualization.\r\n\r\n#Visualization of entries of Q matrix\r\n![Monitor_Q](https://github.com/RobRomijnders/q_learning/blob/master/monitor_Q_values.png?raw=true)\r\nThis monitors random entries of the Q matrix. The lower right plot shows the Exponentially Weighted Moving Average of the convergence criterion, _Q_diff_. With this diagram, we can visually inspect the convergence of the Q matrix from both the random entries and the convergence criterion itself.\r\n#Visualization of epsilon and alpha decay\r\n![Monitor_ea](https://github.com/RobRomijnders/q_learning/blob/master/ea_decay.png?raw=true)\r\nThis plots the decay of epsilon and alpha over the number of steps, k. If you design your own decay function, this script might be a good start for visualizing your decay.\r\n#Visualization of any matrix representing the grid\r\n![Monitor_grid](https://github.com/RobRomijnders/q_learning/blob/master/reward_visualization.png?raw=true)\r\nThis is one example of the visualization functions _visQ.m_ and _visQ_im.m_. These functions visualize any matrix that represents some values in the grid. You can use it to visualize the Q matrix, the N matrix or, in this example, the reward matrix.\r\n\r\n### Further improvements\r\nThis code implements Q-learning at its minimal form. For anyone new to Q-learning, the reward matrices and a 10x10 grid form a good first learning experience.\r\nFurther improvements would be:\r\n * Explore the N matrix. The code saves state-action pairs to a big matrix named _N_. Exploring this matrix will giver you lots of intuition regarding the behavior of epsilon-greedy.\r\n * Explore different values for gamma. The example code in _RL_main_github.m_ calls for gamma at 0.5 and 0.9. You'll see that the optimal policy won't reach final state for gamma = 0.5, but it does for gamma=0.9. The threshold lies at gamma=0.8. Interestingly, the optimal policy at gamma=0.7 does reach final state for reward function _reward_long.mat_. Vice versa, optimal policy at gamma=0.9 is the lowest gamma for which the optimal policy reaches final state with reward function _reward_short.mat_.\r\n * Explore the updates of epsilon and alpha. For this implementation, epsilon and alpha follow the same formula. You might obtain better performances at different formulas. Moreover, you might find even better performances with your own formulas for updating epsilon and alpha. Start with the script _eak.m_ for doing so.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}